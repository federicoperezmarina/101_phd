# 101_phd
This repository is going to explain all the steps given by me in order to do my phd.

A very useful step is to read a metapaper of papers which explains how to read papers. [PDF](paper-reading.pdf)

## Papers
| Paper | Useful |
|---|---|
|[On LLMs-Driven Synthetic Data Generation, Curation, and Evaluation: A Survey](https://github.com/federicoperezmarina/101_phd/tree/main/papers/2406.15126) | :white_check_mark: |
|[DataDreamer: A Tool for Synthetic Data Generation and Reproducible LLM Workflows](https://github.com/federicoperezmarina/101_phd/tree/main/papers/2402.10379) | :white_check_mark: |
|[Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations](https://github.com/federicoperezmarina/101_phd/tree/main/papers/2310.07849) | :white_check_mark: |
|[Scaling Synthetic Data Creation with 1,000,000,000 Personas](https://github.com/federicoperezmarina/101_phd/tree/main/papers/2406.20094) | :x: |
|[The Power of LLM-Generated Synthetic Data for Stance Detection in Online Political Discussion](https://github.com/federicoperezmarina/101_phd/tree/main/papers/2406.12480)| :white_check_mark: |
|[MedSyn: LLM-based Synthetic Medical TextGeneration Framework](https://github.com/federicoperezmarina/101_phd/tree/main/papers/2408.02056)| :white_check_mark: |
|[LLM-Based Synthetic Datasets: Applications and Limitations in Toxicity Detection](https://github.com/federicoperezmarina/101_phd/tree/main/papers/2024trac_16)|:x:|
|[A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications](https://github.com/federicoperezmarina/101_phd/tree/main/papers/2402.07927)| :white_check_mark: |
|[Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://github.com/federicoperezmarina/101_phd/tree/main/papers/2201.11903)| :white_check_mark: |
|[Automatic Chain of Thought Prompting in Large Language Models](https://github.com/federicoperezmarina/101_phd/tree/main/papers/2210.03493)| :white_check_mark: |
|[Self-Consistency Improves Chain of Thought Reasoning in Language Models](https://github.com/federicoperezmarina/101_phd/tree/main/papers/2203.11171)| :white_check_mark: |
|[Large Language Model Guided Tree-of-Thought](https://github.com/federicoperezmarina/101_phd/tree/main/papers/2201.08291)| :white_check_mark: |
|[Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Language Models](https://github.com/federicoperezmarina/101_phd/tree/main/papers/2201.16582)| :white_check_mark: |
|[CHAIN-OF-TABLE: EVOLVING TABLES IN THE REASONING CHAIN FOR TABLE UNDERSTANDING](https://github.com/federicoperezmarina/101_phd/tree/main/papers/2401.04398)| :white_check_mark: |


## Data Generation

### How to generate Synthetic Data?
- With a generic LLM
- With specialized LLM (Model trained for specific tasks)


### What can I use? (Tools)
- We are going to use Ollama in order to start doing our code.

### Prompt Engineering deep dive
- Secuence techniques
    - Chain of Tougth (CoT)
    - Auto-CoT
    - Self-Consistency
    - Tree-of-Thought (ToT)
    - Graph-of-Thought (GoT)

## Data Curation
- Comming soon

## Data Validation
- Comming soon