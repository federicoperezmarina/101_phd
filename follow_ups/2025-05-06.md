# Follow-up — 2025-05-06

## Last Follow-up Actions

1. **Explore LLM limitations**
    - Context limit
    - Losing the context
    - Identify LLM limitations/problems
    - Find a dataset (preferably tabular)
    - Generate output, iterate with dataset
    - Define a methodology:
        - Step 1
        - Step 2
        - Step 3

2. **Develop and group prompt engineering techniques**
    - Sequence-based prompts
    - Tree structure → multiple paths
    - Graph structure → cyclic prompts

---

## Things DONE

### Coding Examples — LLaMA 3.2

**Limitations Observed:**
- Supported languages: English, German, French, Italian, Portuguese, Hindi, Spanish, Thai
- Is the LLM censored?
- Context size: 128K tokens
- Handles input text
- Lacks real-time or private information
- Problems with data types

**Test Scripts:**

| Script             | Language / Test Type | Status | Output / Notes                                                                                      |
|--------------------|----------------------|--------|-----------------------------------------------------------------------------------------------------|
| `llm_limit_1.py`   | English              | ✅     | `"127.0.0.1 - - [15/Jul/2019:13:34:09 +0000] 'GET /index.html HTTP/1.1' 200 1234"`                  |
| `llm_limit_2.py`   | Spanish              | ✅     | `"GET /static/file.txt HTTP/1.1 200 1234"`                                                          |
| `llm_limit_3.py`   | Latin                | ❌     | `"Statice lima feliciter petitionem solum includet, truncum nuntium una linea."`<br>LLM not trained |
| `llm_limit_4.py`   | Censorship test      | ✅     | `"I can't provide information on where to buy illegal drugs..."`                                    |
| `llm_limit_5.py`   | Censorship test      | ✅     | `"I can't provide you with a list of actual users and passwords..."`                                |
| `llm_limit_6.py`   | Large context        | ❌     | Failed to return *Supercalifragilisticoexpialidoso* — model doesn’t crash                           |
| `llm_limit_7.py`   | Large context        | ❌     | Failed to return *Supercalifragilisticoexpialidoso* — model doesn’t crash                           |
| `llm_limit_8.py`   | Large context        | ❌     | Failed to return *Supercalifragilisticoexpialidoso* — model doesn’t crash                           |
| `llm_limit_9.py`   | Small context        | ✅     | `"Supercalifragilisticoexpialidoso"`                                                                |
| `llm_limit_10.py`  | Small context with ValueError       | ✅     | `"Supercalifragilisticoexpialidoso"`                                                 |
| `llm_limit_11.py`  | Small context with Exceed max content      | ✅     | `"Supercalifragilisticoexpialidoso"`                                          |
| `llm_limit_12.py`  | Dataset / CSV        | ❌     | The model is giving a wrong answer, Coelophysis                                       |
| `llm_limit_13.py`  | Dataset / CSV curated| ❌     | The model is giving a wrong answer, Velociraptor                                     |
| `llm_limit_14.py`  | Dataset / CSV curated| ❌     | The model is giving a wrong answer, Velociraptor                                     |
| `llm_limit_15.py`  | Dataset / CSV typed  | ❌     | The anwers is wrong Krillin & Freezer have the same heigth |
| `llm_limit_15.py`  | Dataset / CSV typed  | ❌     | The anwers is wrong Broly is the most heavy |

#### Conclusions
- Large Context Windows will not be able to anwser in the correct way, but it will answer.
- LLM limitations like Language will not be able to answer in the correct way, but it will answer.
- LLM's has tecnhiques to memorize better the information avoiding information not relevant.
- LLM's use attention mechanism
- RAG Architectures: Structured layout, summaries, frequently searched item, consistent formating, bullet points, brake up in blocks, table of contents...
- Ollama has 2048 token window context, but you can modify it in the terminal
- It seems that don't understand the typing (numbers). The llm recieves a text, it is not understanding the types.
- You can only ask for one question otherwise the llm get confused

#### Limits
- Text
    - LLM languages 
    - LLM Censorship
    - LLM Context limit
- Dataset / Table / CSV
    - CSV curated but only string
    - CSV typed
- Prompt 
    - Define the input
        - Json for typing
    - Define the output 
        - Without ambiguity
        - Only one thing
        - Be specific in the output


### Papers

**Large Language Model Guided Tree-of-Thought**
- [Paper](https://github.com/federicoperezmarina/101_phd/tree/main/papers/2201.08291)
- The Tree-of-Thought 
- [Tree of Thought Technique](ToT_technique.png)

**Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Language Models**
- [Paper](https://github.com/federicoperezmarina/101_phd/tree/main/papers/2201.16582)
- Graph of Thought
- [Graph of Thought Technique](GoT_technique.png)
---

## Next Steps

_(To be defined)_