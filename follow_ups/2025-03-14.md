# Follow-up 14-03-2025

## Generation with Ollama
- Generation with Ollama 3.2 (Generic LLM) 
- Prompt engineering:
    - Zero-shot
    - One-shot
    - Few-shot 
    - Multi-step generation

- Examples:
    - [zero-shot prompting with ollama](https://github.com/federicoperezmarina/101_phd/blob/main/code/zero_shot_ollama_3_2.py)
    - [one-shot prompting with ollama](https://github.com/federicoperezmarina/101_phd/blob/main/code/one_shot_ollama_3_2.py)
    - [few-shot prompting with ollama](https://github.com/federicoperezmarina/101_phd/blob/main/code/few_shot_ollama_3_2.py)
    - [multi-step prompting with ollama](https://github.com/federicoperezmarina/101_phd/blob/main/code/multi_step_ollama_3_2.py)

## Structure output
- We should add to the prompt information in order to have a more concise output

## Conditional prompts
- We can add conditionals to the prompt

## Data Validation
- There is a python library validator [Pydantic](https://docs.pydantic.dev/latest/)
- You can use its own validators and also you can create custom.

## Next Actions
- Use another free model
- Use OpenAi | Antropic | Gemini model
- Use open source reasoning model
- Use OpenAi reasoning model

- Abstraer formalizar los diferentes prompts
- Contexto + Formato salida + Instrucci√≥n
